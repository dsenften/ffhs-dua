\section{KMP String-Suchalgorithmus (Knuth-Morris-Pratt)}

\subsection{Einleitung}

Der KMP-Algorithmus (Knuth-Morris-Pratt) ist ein effizienter String-Suchalgorithmus,
der einen Muster-String in einem Text-String sucht. Benannt nach seinen Erfindern
Donald Knuth, Vaughan Pratt und James Morris (1977), ist KMP besonders bemerkenswert,
weil er {\bf garantierte lineare Laufzeit} auch im Worst-Case bietet.

{\bf Kernvorteil}: Im Gegensatz zu naiven Ans√§tzen nutzt KMP eine vorberechnete
Tabelle (DFA - Deterministic Finite Automaton oder Verschiebetabelle),
um {\bf Backtracking im Text zu vermeiden}. Der Text-Index l√§uft nur vorw√§rts,
was zu einer garantierten O(n) Laufzeit f√ºhrt, wobei n die Textl√§nge ist.

{\bf Grundprinzip}: KMP verfolgt prinzipiell die gleiche Idee wie die Konstruktion
eines deterministischen endlichen Automaten, vermeidet jedoch die aufw√§ndige
Konstruktion eines kompletten Automaten und beschr√§nkt sich auf das Wesentliche:
die Suche nach Pr√§fixen des Musters innerhalb des Musters selbst.

\subsection{Hauptmerkmale}

\subsubsection{Vorteile gegen√ºber naiven Algorithmen}

\startitemize[n]
\item {\bf Kein Backtracking im Text}: Der Text-Index l√§uft nur vorw√§rts
\item {\bf Worst-Case $O(n)$ Laufzeit}: Garantierte lineare Laufzeit f√ºr die Suche
\item {\bf Wiederverwendbar}: DFA wird einmal aufgebaut und kann f√ºr beliebig viele
Suchen verwendet werden
\item {\bf Effizient bei wiederholten Zeichen}: Gerade bei Mustern mit Wiederholungen
deutlich schneller als naive Ans√§tze
\stopitemize

\subsubsection{Typische Anwendungsf√§lle}

\startitemize
\item {\bf Textsuche}: Schnelle Suche in grossen Texten (Editoren, Datenbanken)
\item {\bf DNA-Sequenzanalyse}: Suche nach genetischen Mustern
\item {\bf Netzwerk-Intrusion-Detection}: Pattern Matching in Netzwerkpaketen
\item {\bf Plagiatserkennung}: Effiziente Suche nach kopierten Textpassagen
\stopitemize

\subsection{Algorithmus}

\subsubsection{Naive String-Suche (zum Vergleich)}

Die naive String-Suche vergleicht das Muster an jeder Position im Text:

\startpythoncode
def naive_search(pattern: str, text: str) -> int:
    m = len(pattern)
    n = len(text)

    for i in range(n - m + 1):
        j = 0
        while j < m and text[i + j] == pattern[j]:
            j += 1
        if j == m:
            return i  # Gefunden an Position i
    return n  # Nicht gefunden
\stoppythoncode

{\bf Problem}: Bei einem Mismatch springt der Algorithmus nur um 1 Position weiter,
auch wenn mehr Information verf√ºgbar w√§re. Dies f√ºhrt zu O(n √ó m) Laufzeit im Worst-Case.

{\bf Beispiel-Worst-Case}:
\startitemize
\item Text: \type{"aaaaaaaaab"}
\item Muster: \type{"aaab"}
\item Naive Suche macht viele unn√∂tige Vergleiche bei jedem Mismatch
\stopitemize

\subsubsection{KMP-Idee: Verschiebetabelle und DFA}

KMP kann auf zwei √§quivalente Arten verstanden werden:

\startitemize
\item {\bf Verschiebetabelle-Ansatz (H√§berlein)}: Die Verschiebetabelle $P$ speichert
f√ºr jede Position $i$ im Muster die L√§nge des maximalen Pr√§fixes,
das sich vor Position $i$ befindet
\item {\bf DFA-Ansatz (Sedgewick/Wayne)}: KMP baut einen DFA auf, der f√ºr
jeden Zustand (Position im Muster) und jedes m√∂gliche Zeichen den n√§chsten Zustand
bestimmt
\stopitemize

{\bf Kern-Prinzip}: Bei einem Mismatch springt der Algorithmus nicht zur√ºck zum Anfang,
sondern nutzt die bereits gematchten Zeichen, um den optimalen n√§chsten Zustand zu finden.

\subsubsection{Verschiebetabelle-Konstruktion (H√§berlein-Ansatz)}

Die Verschiebetabelle wird analog zur KMP-Suche berechnet, nur dass hier das Muster
im Muster selbst gesucht wird:

\startpythoncode
def VerschTab(M):
    """Berechnet die Verschiebetabelle f√ºr das Muster M."""
    q = -1
    P = [q]  # Verschiebetabelle

    for i in range(1, len(M)):
        # Suche l√§ngsten Pr√§fix vor Position i
        while q >= 0 and M[q] != M[i]:
            q = P[q]  # Fallback √ºber Verschiebetabelle
        q += 1
        P.append(q)

    return P
\stoppythoncode

{\bf Beispiel f√ºr Muster "kakaokaki"}:
\startitemize
\item $P = [-1, 0, 0, 1, 2, 0, 1, 2, 3]$
\item $P[7] = 3$: Die drei Zeichen "kak" vor Position 7 sind ein Pr√§fix
\stopitemize

\subsubsection{DFA-Konstruktion (Sedgewick/Wayne-Ansatz)}

Der DFA wird in O(m √ó R) Zeit aufgebaut:

\startpythoncode
class KMP:
    def __init__(self, pattern: str) -> None:
        self._pattern = pattern
        self._R = 256  # Extended ASCII
        m = len(pattern)

        # Initialisiere DFA (R √ó m Matrix)
        self._dfa = [[0] * m for _ in range(self._R)]

        # Baue DFA auf
        self._dfa[ord(pattern[0])][0] = 1  # Match im Zustand 0

        x = 0  # Restart-Zustand (Fallback bei Mismatch)
        for j in range(1, m):
            # Kopiere Mismatch-F√§lle vom Restart-Zustand
            for c in range(self._R):
                self._dfa[c][j] = self._dfa[c][x]

            # Setze Match-Fall
            self._dfa[ord(pattern[j])][j] = j + 1

            # Update Restart-Zustand
            x = self._dfa[ord(pattern[j])][x]
\stoppythoncode

{\bf Restart-Zustand x}: Simuliert den DFA auf dem Muster selbst, um bei einem
Mismatch den optimalen Fallback-Zustand zu finden.

\subsubsection{Suche mit Verschiebetabelle (H√§berlein-Ansatz)}

\startpythoncode
def KMP(M, T):
    """Knuth-Morris-Pratt Suche mit Verschiebetabelle."""
    P = VerschTab(M)  # Berechne Verschiebetabelle
    erg = []
    q = -1  # Position im Muster (zuletzt erfolgreich gepr√ºft)

    for i in range(len(T)):
        # Bei Mismatch: nutze Verschiebetabelle f√ºr Fallback
        while q >= 0 and M[q + 1] != T[i]:
            q = P[q]

        q += 1  # N√§chste Position im Muster

        # Vollst√§ndiger Match gefunden
        if q == len(M) - 1:
            erg.append(i + 1 - len(M))  # Startposition des Matches
            q = P[q]  # Bereite n√§chste Suche vor

    return erg
\stoppythoncode

\subsubsection{Suche mit DFA (Sedgewick/Wayne-Ansatz)}

Die Suche l√§uft in $O(n)$ Zeit:

\startpythoncode
def search(self, text: str) -> int:
    n = len(text)
    m = len(self._pattern)

    i = 0  # Text-Index
    j = 0  # Muster-Index (Zustand im DFA)

    # Durchlaufe Text (nur vorw√§rts, kein Backtracking!)
    while i < n and j < m:
        j = self._dfa[ord(text[i])][j]  # N√§chster Zustand
        i += 1  # Text-Index l√§uft nur vorw√§rts

    # Gefunden wenn j == m (Endzustand erreicht)
    if j == m:
        return i - m
    return n  # Nicht gefunden
\stoppythoncode

{\bf Wichtig}: Der Text-Index \type{i} l√§uft nur vorw√§rts. Es gibt kein Backtracking!

\subsection{Python-Implementierung}

\subsubsection{Klasse KMP}

\startpythoncode
from collections.abc import Iterator

class KMP:
    """Knuth-Morris-Pratt String-Suchalgorithmus.

    Implementiert effiziente String-Suche durch Verwendung eines
    Deterministischen Finiten Automaten (DFA). Der DFA wird einmal
    beim Konstruktor aufgebaut und kann dann f√ºr beliebig viele
    Suchen wiederverwendet werden.
    """

    def __init__(self, pattern: str) -> None:
        """Initialisiert den KMP-Algorithmus mit einem Suchmuster."""
        if pattern is None:
            raise ValueError("Muster darf nicht None sein")
        if not pattern:
            raise ValueError("Muster darf nicht leer sein")

        self._pattern = pattern
        self._R = 256  # Extended ASCII
        # ... DFA-Aufbau ...

    @property
    def pattern(self) -> str:
        """Gibt das Suchmuster zur√ºck."""
        return self._pattern

    def search(self, text: str) -> int:
        """Sucht das Muster im gegebenen Text.

        Returns:
            Index der ersten √úbereinstimmung, oder len(text) wenn nicht gefunden
        """
        # ... Suche mit DFA ...

    def search_all(self, text: str) -> Iterator[int]:
        """Findet alle Vorkommen des Musters im Text.

        Returns:
            Iterator √ºber alle Match-Positionen
        """
        pos = 0
        while pos <= len(text) - len(self._pattern):
            remaining_text = text[pos:]
            offset = self.search(remaining_text)

            if offset < len(remaining_text):
                yield pos + offset
                pos += offset + 1  # Weiter nach erstem Match
            else:
                break

    def count(self, text: str) -> int:
        """Z√§hlt alle Vorkommen des Musters im Text."""
        return sum(1 for _ in self.search_all(text))
\stoppythoncode

\subsubsection{Verwendungsbeispiele}

\startpythoncode
# Einfache Suche
kmp = KMP("NEEDLE")
position = kmp.search("HAYSTACK WITH NEEDLE IN IT")
print(position)  # 14

# Nicht gefunden
kmp = KMP("xyz")
position = kmp.search("abcdef")
print(position)  # 6 (len(text))

# Alle Vorkommen finden
kmp = KMP("ab")
for pos in kmp.search_all("ababab"):
    print(pos)  # 0, 2, 4

# Vorkommen z√§hlen
kmp = KMP("the")
count = kmp.count("the quick brown fox jumps over the lazy dog")
print(count)  # 2
\stoppythoncode

\subsection{Komplexit√§tsanalyse}

\subsubsection{Laufzeitanalyse (Amortisierte Analyse)}

{\bf Kernbeobachtung}: Die Variable $q$ (Muster-Position) kann nicht bei jedem
Durchlauf der Hauptschleife um $m$ Werte erniedrigt werden. Die while-Schleife
stellt sicher, dass $q$ nur bis zum Wert $-1$ erniedrigt werden kann.
Um $q$ erneut zu erniedrigen, muss es zun√§chst erh√∂ht worden sein.

{\bf Amortisierte Analyse}:
\startitemize
\item Jede Erh√∂hung von $q$ geht mit einer Erh√∂hung von $i$ (Text-Index) einher
\item Schlimmster Fall: $q$ wird immer in Einerschritten erniedrigt und danach wieder erh√∂ht
\item Insgesamt: $n$ Schritte "nach oben" + $n$ Schritte "nach unten" = $2n$ Schritte
\item {\bf Resultat}: Worst-Case-Komplexit√§t von $O(2n) = O(n)$
\stopitemize

\subsubsection{Komplexit√§tstabelle}

\placetable[here][tab:kmp-complexity]{Komplexit√§tsanalyse KMP-Operationen}
\starttabulate[|l|l|l|l|]
\HL
\NC {\bf Operation} \NC {\bf Laufzeit} \NC {\bf Speicher} \NC {\bf Bemerkung} \NC\NR
\HL
\NC Verschiebetabelle-Aufbau \NC $O(m)$     \NC $O(m)$     \NC $m$ = Muster-L√§nge \NC\NR
\NC DFA-Aufbau.              \NC $O(m √ó R)$ \NC $O(m √ó R)$ \NC $R$ = Alphabet-Gr√∂sse (256) \NC\NR
\NC \type{search(text)}      \NC $O(n)$     \NC $O(1)$     \NC $n$ = Text-L√§nge, garantiert! \NC\NR
\NC \type{search_all(text)}  \NC $O(n)$     \NC $O(1)$     \NC Bei $k$ Matches: $O(n + k √ó m)$ \NC\NR
\NC \type{count(text)}       \NC $O(n)$     \NC $O(1)$     \NC Bei $k$ Matches: $O(n + k √ó m)$ \NC\NR
\HL
\stoptabulate

{\bf Wichtig}:
\startitemize
\item Die Suche hat {\bf garantierte $O(n)$ Laufzeit}, unabh√§ngig vom Muster
\item Kein Backtracking im Text ($i$ l√§uft nur vorw√§rts)
\item Der DFA-Speicher ist $O(m √ó R)$, bei $R=256$ sind das $256 √ó m$ Integer
\item Die Verschiebetabelle ben√∂tigt nur $O(m)$ Speicher
\stopitemize

\subsubsection{Vergleich mit naiven Algorithmen}

{\bf Worst-Case Beispiel}:
\startitemize
\item Text: \type{"aaaaaaaaaaaab"} (n Zeichen)
\item Muster: \type{"aaaab"} (m Zeichen)
\stopitemize

\placetable[here][tab:kmp-vs-naive]{Vergleich KMP vs. naive Suche}
\starttabulate[|l|l|l|]
\HL
\NC {\bf Algorithmus} \NC {\bf Laufzeit} \NC {\bf Vergleiche} \NC\NR
\HL
\NC Naive Suche \NC O(n √ó m) \NC (n - m + 1) √ó m \NC\NR
\NC KMP \NC O(n) \NC n \NC\NR
\HL
\stoptabulate

Bei n=1.000.000 und m=100:
\startitemize
\item Naive: ~100.000.000 Vergleiche
\item KMP: ~1.000.000 Vergleiche (100√ó schneller!)
\stopitemize

\subsection{Limitierungen}

\subsubsection{Extended ASCII (256 Zeichen)}

Die aktuelle Implementierung verwendet eine 256-Element DFA-Tabelle f√ºr Extended ASCII:

\startpythoncode
self._R = 256  # Extended ASCII
\stoppythoncode

{\bf Limitation}: Unicode-Zeichen mit \type{ord() > 255} (z.B. Emojis wie üòÄ mit ord=128512) f√ºhren zu einem \type{IndexError}.

{\bf L√∂sung} (f√ºr volle Unicode-Unterst√ºtzung):
\startpythoncode
# Alternative: Dictionary-basierte DFA statt Array
self._dfa = [{} for _ in range(m)]  # Speichert nur tats√§chlich vorkommende Zeichen
\stoppythoncode

\subsubsection{Speicherverbrauch}

Bei langen Mustern (z.B. m=10.000) ben√∂tigt der DFA:
\startitemize
\item Array-basiert: 256 √ó 10.000 √ó 4 Bytes ‚âà 10 MB
\item Dictionary-basiert: Deutlich weniger, abh√§ngig vom Alphabet
\stopitemize

F√ºr sehr lange Muster k√∂nnen alternative Algorithmen wie {\bf Boyer-Moore} effizienter sein.

\subsection{Vergleich mit anderen String-Suchalgorithmen}

\placetable[here][tab:string-algorithms-comparison]{Vergleich String-Suchalgorithmen}
\starttabulate[|l|l|l|l|l|l|]
\HL
\NC {\bf Algorithmus} \NC {\bf Preprocessing} \NC {\bf Suche (Average)} \NC {\bf Suche (Worst)} \NC {\bf Speicher} \NC {\bf Bemerkung} \NC\NR
\HL
\NC Naive \NC O(1) \NC O(n √ó m) \NC O(n √ó m) \NC O(1) \NC Einfach, aber langsam \NC\NR
\NC {\bf KMP} \NC O(m √ó R) \NC O(n) \NC O(n) \NC O(m √ó R) \NC Kein Backtracking, garantiert O(n) \NC\NR
\NC Boyer-Moore \NC O(m + R) \NC O(n/m) \NC O(n √ó m) \NC O(m + R) \NC Oft schneller als KMP \NC\NR
\NC Rabin-Karp \NC O(m) \NC O(n + m) \NC O(n √ó m) \NC O(1) \NC Nutzt Hashing \NC\NR
\HL
\stoptabulate

{\bf KMP vs. Boyer-Moore}:
\startitemize
\item KMP: Garantierte O(n) Laufzeit, kein Backtracking
\item Boyer-Moore: Durchschnittlich schneller (O(n/m)), aber O(n √ó m) im Worst-Case
\stopitemize

{\bf Wann KMP nutzen}:
\startitemize
\item Garantierte Worst-Case Performance gefordert
\item Muster wird oft wiederverwendet (DFA-Aufbau amortisiert)
\item Text mit vielen Wiederholungen (DNA-Sequenzen, komprimierte Daten)
\stopitemize

\subsection{Zusammenfassung}

{\bf KMP (Knuth-Morris-Pratt)} ist ein fundamentaler String-Suchalgorithmus mit folgenden Charakteristika:

{\bf St√§rken}:
\startitemize
\item Garantierte O(n) Laufzeit, auch im Worst-Case
\item Kein Backtracking im Text (i l√§uft nur vorw√§rts)
\item Wiederverwendbarer DFA f√ºr mehrfache Suchen
\item Effizient bei Mustern mit selbst-√ºberlappenden Pr√§fixen
\stopitemize

{\bf Zu beachten}:
\startitemize
\item Preprocessing ben√∂tigt O(m √ó R) Zeit und Speicher
\item Bei kleinen Alphabeten (DNA: A,C,G,T) sehr speichereffizient
\item Bei grossen Alphabeten (Unicode) kann Dictionary-basierte DFA besser sein
\item Boyer-Moore ist in der Praxis oft schneller, aber ohne Worst-Case Garantie
\stopitemize

{\bf Einsatzempfehlung}: KMP ist die beste Wahl f√ºr Anwendungen, die {\bf garantierte lineare Laufzeit} ben√∂tigen, wie Echtzeitsysteme, Netzwerk-Intrusion-Detection oder DNA-Sequenzanalyse. F√ºr allgemeine Textsuche kann Boyer-Moore schneller sein.

\blank[medium]

{\it Quelle: Praktische Algorithmik mit Python von Tobias H√§berlein, Abschnitt 7.3 "Der Knuth-Morris-Pratt-Algorithmus" und Algorithms, 4th Edition von Sedgewick & Wayne, Abschnitt 5.3 "Substring Search"}
